# 1 - 网络层数问题

- 网络层数：输入层 + 隐藏层 + 输出层
- 全连接层数：仅计算全连接层
- ReLU层数：全连接层数 - 1（最后一个个全连接层不带ReLU）
- 例如：1层输入层 + 2层隐藏层 + 1层输出层。网络层数=4，全连接层数=3，ReLU层数=2

# 2 - 创建网络层

创建全连接层：

```
import torch.nn as nn
fc = nn.Linear(左边节点数, 右边节点数)
```

进行运算：

```
y = fc(x)
```

进行带有ReLU的全连接计算：

```
relu = nn.ReLU()  # Create ReLU Instance, remember ()
x = fc(x)
y = relu(x)       # or y = nn.ReLU()(x), remember double ()
```

创建网络类：

```
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 定义全连接层

    def forward(self, x):
        # 进行带有ReLU的全连接计算
        return x
```

创建网络实例：

```
net = Net()
```

# 3 - 训练

## 生成数据集

```
torch.randn(x, y)
```

randn有两个参数，说明会生成一个二维张量。

```
x_train = torch.randn(10, 5)
y_train = torch.randn(10, 4)
```

这里，输入层5个节点，输出层4个节点。

10是什么意思？是数据集容量为10的意思。一定要把x和y的第一维度作为数据集的个数（容量），这样`pytorch`的`nn.MSELoss`和`optim.SGD`才会支持。

## 定义损失函数和优化器

```
import torch.nn as nn
import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)
```

criterion实质就是Loss函数，optimizer实质就是自动的梯度下降求导器。`net.parameters()`会返回权重矩阵的迭代器。

## 训练迭代的通用思路

用网络示例进行预测：

```
y_hat = net(x_train)
```

计算损失函数：

```
loss = criterion(y_hat, y_train)
```

反向传播通用写法：

```
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

获取Loss的标量值：

```
loss.item()
```

迭代的代码：

```python
for epoch in range(1000):
    # 前向传播
    y_hat = net(x_train)
    loss = criterion(y_hat, y_train)

    # 反向传播
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print('Epoch [%s/%s], Loss: %s' % (epoch + 1, 1000, loss.item()))
        # loss.item()是用来获取张量loss中的单个数值的函数。
```

思路：

- 用网络做一次预测
- 计算Loss函数
- 清空梯度
- 反向传播
- 梯度下降

