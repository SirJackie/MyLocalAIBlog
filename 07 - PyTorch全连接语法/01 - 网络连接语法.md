# 1

- 网络层数：输入层 + 隐藏层 + 输出层
- 全连接层数：仅计算全连接层
- ReLU层数：全连接层数 - 1（最后一个个全连接层不带ReLU）
- 例如：1层输入层 + 2层隐藏层 + 1层输出层。网络层数=4，全连接层数=3，ReLU层数=2

# 2

创建全连接层：

```
import torch.nn as nn
fc = nn.Linear(左边节点数, 右边节点数)
```

进行运算：

```
y = fc(x)
```

进行带有ReLU的全连接计算：

```
relu = nn.ReLU()  # Create ReLU Instance, remember ()
x = fc(x)
y = relu(x)       # or y = nn.ReLU()(x), remember double ()
```

创建网络类：

```
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 定义全连接层

    def forward(self, x):
        # 进行带有ReLU的全连接计算
        return x
```

创建网络实例：

```
net = Net()
```

